# Multiple Regression Analysis

## ğŸ¯ Multiple Predictors

Extending simple regression to multiple independent variables.

---

## ğŸ“ The Model

**Y = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚–Xâ‚– + Îµ**

Where:
- Y = dependent variable
- Xâ‚, Xâ‚‚, ..., Xâ‚– = independent variables
- Î²â‚€ = intercept
- Î²â‚, Î²â‚‚, ..., Î²â‚– = slopes
- Îµ = error term

---

## ğŸ” Interpretation

### Slope Coefficients

**Î²â±¼** = change in Y for 1-unit change in Xâ±¼, **holding other X's constant**

**Also called**:
- Partial effect
- Marginal effect
- Ceteris paribus effect

### Example
**Wage = Î²â‚€ + Î²â‚(Education) + Î²â‚‚(Experience) + Îµ**

**Î²â‚** = effect of education, **controlling for** experience

---

## ğŸ“Š RÂ² in Multiple Regression

**RÂ²** = proportion of variance in Y explained by ALL X's

**Adjusted RÂ²** = RÂ² - [(k/(n-k-1)) Ã— (1-RÂ²)]
- Penalizes adding variables
- Use when comparing models with different k

---

## ğŸ¯ F-Test for Overall Significance

**Tests**: Hâ‚€: Î²â‚ = Î²â‚‚ = ... = Î²â‚– = 0

**Formula**: F = [RÂ²/k] / [(1-RÂ²)/(n-k-1)]

**Interpretation**: Does model explain significant variance?

---

## ğŸ”‘ Key Takeaways

1. **Multiple X's** predict single Y
2. **Coefficients** are partial effects
3. **Control** for confounds by including them
4. **RÂ²** shows total variance explained
5. **F-test** for overall model significance

---

*Part of: [[00-Index|Business Research Methods Course Notes]]*
