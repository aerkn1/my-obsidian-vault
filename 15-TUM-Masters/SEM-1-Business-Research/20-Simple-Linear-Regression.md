# Simple Linear Regression

## üéØ The Goal of Regression

**Find the best-fitting line** through a scatterplot to predict Y from X.

```mermaid
graph LR
    A[Independent Variable<br/>X] -->|Linear relationship| B[Dependent Variable<br/>Y]
    
    C[Predict Y<br/>from X] --> B
```

---

## üìê The Model

### Mathematical Form

**Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ**

Where:
- **Y**: Dependent variable (outcome)
- **X**: Independent variable (predictor)
- **Œ≤‚ÇÄ**: Intercept (value of Y when X=0)
- **Œ≤‚ÇÅ**: Slope (change in Y for 1-unit change in X)
- **Œµ**: Error term (random deviation)

---

## üìä OLS Estimation

### Ordinary Least Squares (OLS)

**Principle**: Minimize sum of squared residuals

**Minimize**: Œ£(y·µ¢ - ≈∑·µ¢)¬≤ = Œ£e·µ¢¬≤

```mermaid
graph TD
    A[Data Points] --> B[OLS Algorithm]
    B --> C[Find Œ≤‚ÇÄ and Œ≤‚ÇÅ]
    C --> D[That minimize<br/>Œ£(residuals)¬≤]
    D --> E[Best-Fitting Line]
    
    style E fill:#d4edda
```

---

### The Formulas

**Slope**: Œ≤‚ÇÅ = Cov(X,Y) / Var(X)

**Alternative**: Œ≤‚ÇÅ = r √ó (s·µß/s‚Çì)

**Intercept**: Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ

**Key Insight**: Line always passes through (xÃÑ, »≥)

---

## üíº Example Calculation

**Given**:
- Correlation: r = 0.25
- SD of SCORE (Y): s·µß = 7
- SD of ATTEND (X): s‚Çì = 3
- Mean SCORE: »≥ = 75
- Mean ATTEND: xÃÑ = 20

**Find**: Regression equation

**Solution**:

**Step 1**: Calculate Œ≤‚ÇÅ
Œ≤‚ÇÅ = r √ó (s·µß/s‚Çì) = 0.25 √ó (7/3) = 0.583

**Step 2**: Calculate Œ≤‚ÇÄ
Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ = 75 - 0.583(20) = 63.34

**Equation**: SCORE = 63.34 + 0.583(ATTEND)

**Interpretation**:
- Each additional class attended ‚Üí 0.583 points higher score
- Student who attends 0 classes ‚Üí predicted score of 63.34

---

## üìà Interpretation Guide

### The Slope (Œ≤‚ÇÅ)

**Interpretation**: "For every 1-unit increase in X, Y changes by Œ≤‚ÇÅ units"

**Examples**:
- Œ≤‚ÇÅ = 5: Each year of education ‚Üí ‚Ç¨5,000 more salary
- Œ≤‚ÇÅ = -2: Each hour of TV ‚Üí 2 points lower GPA
- Œ≤‚ÇÅ = 0.5: Each dollar of advertising ‚Üí 50 cents more sales

**Sign**:
- Œ≤‚ÇÅ > 0: Positive relationship
- Œ≤‚ÇÅ < 0: Negative relationship  
- Œ≤‚ÇÅ = 0: No relationship

---

### The Intercept (Œ≤‚ÇÄ)

**Interpretation**: "Predicted Y when X = 0"

**Often Not Meaningful**:
- If X = 0 is impossible/unrealistic
- Example: Wage = 50,000 + 5,000(Years Education)
  - Œ≤‚ÇÄ = 50,000 (wage with 0 education)
  - Extrapolation beyond data!

**Sometimes Meaningful**:
- If X = 0 is realistic
- Example: Sales = 1000 + 50(Advertising)
  - Œ≤‚ÇÄ = 1000 (sales with $0 advertising)

---

## üìä R-Squared (R¬≤)

### Definition

**R¬≤ = Proportion of variance in Y explained by X**

**Formula**: R¬≤ = SSE/SST = 1 - SSR/SST

Where:
- SST = Total Sum of Squares = Œ£(y·µ¢ - »≥)¬≤
- SSE = Explained Sum of Squares = Œ£(≈∑·µ¢ - »≥)¬≤
- SSR = Residual Sum of Squares = Œ£(y·µ¢ - ≈∑·µ¢)¬≤

**Relationship**: SST = SSE + SSR

---

### Interpretation

**R¬≤ = 0.35** means:
- 35% of variance in Y explained by X
- 65% due to other factors

**Range**: 0 ‚â§ R¬≤ ‚â§ 1
- R¬≤ = 0: X explains nothing
- R¬≤ = 1: X explains everything (perfect fit)

**What R¬≤ Does NOT Tell**:
- ‚ùå Whether coefficients are significant
- ‚ùå Whether model is correctly specified
- ‚ùå Whether relationship is causal

---

## üéØ Residuals

### Definition

**Residual** (e·µ¢) = Observed - Predicted = y·µ¢ - ≈∑·µ¢

**Interpretation**:
- Positive residual: Overestimated Y
- Negative residual: Underestimated Y
- Residual = prediction error

```mermaid
graph TD
    A[Actual Value<br/>y·µ¢ = 80] --> C[Residual<br/>e·µ¢ = 5]
    B[Predicted Value<br/>≈∑·µ¢ = 75] --> C
```

---

### Error Term vs. Residual

**Error term (Œµ)**: Population concept, unobserved
**Residual (e)**: Sample estimate of error

---

## üîç Hypothesis Testing

### Testing Œ≤‚ÇÅ

**Null Hypothesis**: H‚ÇÄ: Œ≤‚ÇÅ = 0 (no relationship)
**Alternative**: H‚ÇÅ: Œ≤‚ÇÅ ‚â† 0 (relationship exists)

**t-statistic**: t = Œ≤ÃÇ‚ÇÅ / SE(Œ≤ÃÇ‚ÇÅ)

**Decision**:
- If |t| large and p < 0.05 ‚Üí Reject H‚ÇÄ ‚Üí Œ≤‚ÇÅ significant
- If p ‚â• 0.05 ‚Üí Fail to reject ‚Üí Œ≤‚ÇÅ not significant

---

### Confidence Interval

**95% CI**: Œ≤ÃÇ‚ÇÅ ¬± 1.96 √ó SE(Œ≤ÃÇ‚ÇÅ)

**Interpretation**:
- If CI includes 0 ‚Üí Not significant
- If CI excludes 0 ‚Üí Significant

---

## üéì Assumptions

### 5 Key OLS Assumptions

1. **Linearity**: Relationship is linear
2. **Independence**: Observations independent
3. **Homoscedasticity**: Constant error variance
4. **Normality**: Errors normally distributed
5. **No perfect collinearity**: (Not issue in simple regression)

---

## üíº Complete Example

**Research Question**: Does study time affect exam scores?

**Data**: 100 students
- X = Study hours
- Y = Exam score (0-100)

**Results**:
- r = 0.60
- xÃÑ = 10 hours, s‚Çì = 3
- »≥ = 70 points, s·µß = 15

**Calculate Regression**:

Œ≤‚ÇÅ = 0.60 √ó (15/3) = 3.0
Œ≤‚ÇÄ = 70 - 3.0(10) = 40

**Equation**: Score = 40 + 3.0(Hours)

**Interpretation**:
- Each additional hour of study ‚Üí 3 points higher
- Student who studies 0 hours ‚Üí predicted score of 40

**R¬≤** = r¬≤ = 0.60¬≤ = 0.36
- 36% of score variance explained by study time

**Prediction**:
- Study 15 hours: Score = 40 + 3(15) = 85

---

## üîë Key Takeaways

1. **OLS minimizes** sum of squared residuals
2. **Œ≤‚ÇÅ** = slope = change in Y per unit change in X
3. **Œ≤‚ÇÄ** = intercept = Y when X=0
4. **Line passes through** (xÃÑ, »≥)
5. **R¬≤** = proportion of variance explained
6. **Residual** = observed - predicted
7. **Test Œ≤‚ÇÅ** using t-statistic

---

*Part of: [[00-Index|Business Research Methods Course Notes]]*
